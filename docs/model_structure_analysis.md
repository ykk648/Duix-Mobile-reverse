# MobileNet.py vs NCNN æ¨¡å‹ç»“æ„å¯¹æ¯”åˆ†æ

## ğŸ“Š æ€»ä½“å¯¹æ¯”

| é¡¹ç›® | NCNN æ¨¡å‹ï¼ˆå®é™…ï¼‰ | PyTorch å¤ç° | çŠ¶æ€ |
|------|------------------|--------------|------|
| æ€»å±‚æ•° | 165 å±‚ | ~ç±»ä¼¼ | âš ï¸ éœ€éªŒè¯ |
| è¾“å…¥ | audio(256Ã—20Ã—1) + face(WÃ—HÃ—6) | audio(256Ã—20) + face(WÃ—HÃ—6) | âœ… åŒ¹é… |
| è¾“å‡º | WÃ—HÃ—3 (TanH) | WÃ—HÃ—3 (Sigmoid) | âŒ **æ¿€æ´»å‡½æ•°ä¸åŒ** |
| å½’ä¸€åŒ– | GroupNorm | InstanceNorm2d/BatchNorm2d | âŒ **ç±»å‹ä¸åŒ** |

---

## ğŸµ éŸ³é¢‘ç¼–ç å™¨å¯¹æ¯”

### NCNN å®é™…ç»“æ„

```
Input: audio [1, 256, 20]

1. Conv(1â†’16, k3, s2, p0)           0=16 1=3 2=1 3=2 4=0
   â†’ ReLU
   â†’ [16, 128, 10]

2. Conv(16â†’32, k3, s2, p0)          0=32 1=3 2=1 3=2 4=0
   â†’ ReLU
   â†’ [32, 64, 5]

3. Conv(32â†’32, k3, s1, p1)          0=32 1=3 2=1 3=1 4=1
   + Residual (Split + Add)
   â†’ ReLU
   â†’ [32, 64, 5]

4. Conv(32â†’64, k3, s2, p1)          0=64 1=3 2=1 3=2 4=1
   â†’ ReLU
   â†’ [64, 32, 3]

5. Conv(64â†’128, k3, s2, p1)         0=128 1=3 2=1 3=2 4=1
   â†’ ReLU
   â†’ [128, 16, 2]

6. Conv(128â†’128, k3, s2, p1, dilation=2)    13=2 (dilation)
   â†’ ReLU
   â†’ [128, 8, 1]

7. Conv(128â†’128, k3, s2, p2, dilation=2)    13=2 14=2 (dilation, pad)
   â†’ ReLU
   â†’ [128, 4, 1]

8. Conv(128â†’128, k3, s1, p1)        0=128 1=3 2=1 3=1 4=1
   + Residual
   â†’ ReLU
   â†’ [128, 4, 1]

æœ€ç»ˆè¾“å‡º: [128, 4, 1] æˆ–ç»è¿‡è¿›ä¸€æ­¥å¤„ç†
```

### PyTorch å¤ç°ç»“æ„

```python
self.audio_encoder = nn.Sequential(
    Conv2d(1, 32, kernel_size=3, stride=2, padding=1),         # [B, 32, 128, 10]  âŒ ç¼ºå°‘ 1â†’16â†’32
    Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),
    
    Conv2d(32, 64, kernel_size=3, stride=2, padding=1),        # [B, 64, 64, 5]
    Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),  # âŒ å¤šä½™çš„å±‚
    
    Conv2d(64, 128, kernel_size=3, stride=2, padding=1),       # [B, 128, 32, 3]
    Conv2d(128, 128, kernel_size=3, stride=2, padding=1),      # [B, 128, 16, 2]
    Conv2d(128, 128, kernel_size=3, stride=2, padding=1),      # [B, 128, 8, 1]  âŒ ç¼ºå°‘ dilation
    Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),
    
    nn.AdaptiveAvgPool2d((5, 5)),                              # [B, 128, 5, 5]
)
```

### âŒ ä¸»è¦å·®å¼‚

| å±‚ | NCNN | PyTorch | é—®é¢˜ |
|----|------|---------|------|
| ç¬¬1å±‚ | Conv(1â†’16, s2, p0) | Conv(1â†’32, s2, p1) | âš ï¸ **é€šé“æ•°ã€paddingä¸åŒ** |
| ç¬¬2å±‚ | Conv(16â†’32, s2, p0) | â€” | âŒ **PyTorchç¼ºå°‘æ­¤å±‚** |
| ç¬¬4å±‚å | â€” | Conv(64â†’64, residual) | âŒ **PyTorchå¤šå‡ºæ­¤å±‚** |
| ç¬¬6-7å±‚ | Conv with **dilation=2** | Conv without dilation | âŒ **ç¼ºå°‘ç©ºæ´å·ç§¯** |
| æœ€å | ä¸ä½¿ç”¨æ± åŒ– | AdaptiveAvgPool2d((5,5)) | âš ï¸ **å¤„ç†æ–¹å¼ä¸åŒ** |

---

## ğŸ–¼ï¸ å›¾åƒç¼–ç å™¨å¯¹æ¯”

### NCNN å®é™…ç»“æ„ï¼ˆMobileNetV2ï¼‰

```
Input: face [W, H, 6]

1. Padding(1,1,1,1, type=2)          pad_185
   â†’ [W+2, H+2, 6]

2. Conv(6â†’16, k3, s1, p0, groups=1)  convrelu_6: 0=16 1=3 7=1
   â†’ ReLU
   â†’ [W, H, 16]

3. Conv(16â†’32, k3, s2, p1)           convrelu_7: 0=32 1=3 3=2 4=1
   â†’ ReLU
   â†’ [W/2, H/2, 32]

4. ç¬¬ä¸€ä¸ª Inverted Residual Block:
   DepthWise(32, k3, s1, p1) + Conv(32â†’16, k1)
   â†’ GroupNorm â†’ ReLU
   
5. åç»­ MobileNetV2 æ ‡å‡†ç»“æ„
   - InvertedResidual(16â†’24, expand_ratio=6, stride=2)
   - InvertedResidual(24â†’24, expand_ratio=6, stride=1) Ã—2
   - InvertedResidual(24â†’32, expand_ratio=6, stride=2)
   - InvertedResidual(32â†’32, expand_ratio=6, stride=1) Ã—3
   - InvertedResidual(32â†’64, expand_ratio=6, stride=2)
   - InvertedResidual(64â†’64, expand_ratio=6, stride=1) Ã—4
   - InvertedResidual(64â†’96, expand_ratio=6, stride=1) Ã—3
   - InvertedResidual(96â†’160, expand_ratio=6, stride=2)
   - InvertedResidual(160â†’320, expand_ratio=6, stride=1)
```

### PyTorch å¤ç°ç»“æ„

```python
# backbone ç¬¬ä¸€å±‚
self.backbone.features[0] = conv_bn(6, input_channel, 2, norm_type)
# input_channel = 32 (é»˜è®¤)

# å®é™…ç¬¬ä¸€å±‚ï¼š
Conv2d(6, 32, 3, 2, 1)  # stride=2
```

### âŒ ä¸»è¦å·®å¼‚

| é¡¹ç›® | NCNN | PyTorch | é—®é¢˜ |
|------|------|---------|------|
| é¢„å¤„ç† | Padding(1,1,1,1) å† Conv(s1) | æ— paddingï¼Œç›´æ¥ Conv(s2) | âš ï¸ **é¢„å¤„ç†æ–¹å¼ä¸åŒ** |
| ç¬¬1å±‚ | Conv(6â†’16, k3, s1, p0) | Conv(6â†’32, k3, s2, p1) | âŒ **é€šé“æ•°ã€strideã€paddingéƒ½ä¸åŒ** |
| ç¬¬2å±‚ | Conv(16â†’32, k3, s2, p1) | â€” | âŒ **PyTorchå·²åœ¨ç¬¬1å±‚å®Œæˆ** |

---

## ğŸ”„ è§£ç å™¨å¯¹æ¯”

### NCNN å®é™…ç»“æ„

```
è§£ç å™¨ä¸»è¦ç»„æˆï¼š
1. Deconvolution Ã—6 (ä¸Šé‡‡æ ·)
2. Concat (with skip connections)
3. Convolution + ConvolutionDepthWise blocks
4. GroupNorm + ReLU

è¾“å‡ºå±‚ï¼š
- Padding(1,1,1,1)
- Conv(8â†’3, k3, s1)
- TanH æ¿€æ´»
```

### PyTorch å¤ç°ç»“æ„

```python
# è§£ç å™¨
self.dconv0 = nn.ConvTranspose2d(128, 128, kernel_size=1, stride=1, padding=0)
self.invres0 = InvertedResidual(448, 96, 1, 6, norm_type)

self.dconv1 = nn.ConvTranspose2d(96, 96, 3, padding=1, stride=2, output_padding=1)
self.invres1 = InvertedResidual(192, 96, 1, 6, norm_type)

self.dconv2 = nn.ConvTranspose2d(96, 32, 3, padding=1, stride=2, output_padding=1)
self.invres2 = InvertedResidual(64, 32, 1, 6, norm_type)

self.dconv3 = nn.ConvTranspose2d(32, 24, 3, padding=1, stride=2, output_padding=1)
self.invres3 = InvertedResidual(48, 24, 1, 6, norm_type)

self.dconv4 = nn.ConvTranspose2d(24, 16, 3, padding=1, stride=2, output_padding=1)
self.invres4 = InvertedResidual(32, 16, 1, 6, norm_type)

self.dconv5 = nn.ConvTranspose2d(16, 8, 3, padding=1, stride=2, output_padding=1)

# è¾“å‡ºå±‚
self.conv_last = nn.Conv2d(8, 3, 1)
self.conv_score = nn.Conv2d(3, 3, 1)
x = torch.sigmoid(x)  # âŒ æ¿€æ´»å‡½æ•°é”™è¯¯
```

### âŒ ä¸»è¦å·®å¼‚

| é¡¹ç›® | NCNN | PyTorch | é—®é¢˜ |
|------|------|---------|------|
| è¾“å‡ºæ¿€æ´» | **TanH** (èŒƒå›´ [-1, 1]) | **Sigmoid** (èŒƒå›´ [0, 1]) | âŒ **æ¿€æ´»å‡½æ•°ä¸åŒï¼Œå½±å“è®­ç»ƒå’Œæ¨ç†** |
| æœ€åä¸€å±‚ | Conv(3â†’3, k1) | Conv(3â†’3, k1) | âœ… åŒ¹é… |
| Padding | æœ‰ Padding å±‚ | æ—  | âš ï¸ å¯èƒ½å½±å“è¾¹ç¼˜æ•ˆæœ |

---

## ğŸ“ å½’ä¸€åŒ–å±‚å¯¹æ¯”

### NCNN ä½¿ç”¨ GroupNorm

```
GroupNorm gn_59  1 1 27 28 0=1 1=24 2=1.000000e-05 3=1
å‚æ•°è¯´æ˜:
- 0=1: num_groups = 1 (ç›¸å½“äº LayerNorm)
- 1=24: channels = 24
- 2=1e-05: epsilon
- 3=1: affine = True
```

### PyTorch ä½¿ç”¨ InstanceNorm2d/BatchNorm2d

```python
# MobileNet.py ä¸­
norm_type = nn.InstanceNorm2d  # é»˜è®¤
# æˆ– nn.BatchNorm2d

# InvertedResidual ä¸­
norm_type(hidden_dim)
```

### âŒ å½’ä¸€åŒ–å·®å¼‚

| ç‰¹æ€§ | GroupNorm (NCNN) | InstanceNorm2d (PyTorch) | å½±å“ |
|------|------------------|--------------------------|------|
| ä½œç”¨èŒƒå›´ | æ¯ä¸ª group | æ¯ä¸ª instance | âš ï¸ ç»Ÿè®¡é‡ä¸åŒ |
| Batchä¾èµ– | âŒ å¦ | âŒ å¦ | âœ… éƒ½ä¸ä¾èµ–batch |
| æ¨ç†æ¨¡å¼ | ç›¸åŒ | ç›¸åŒ | âœ… æ¨ç†æ—¶è¡Œä¸ºä¸€è‡´ |
| è®­ç»ƒè¡Œä¸º | ä¸åŒ | ä¸åŒ | âŒ **è®­ç»ƒæ—¶éœ€è¦è½¬æ¢** |

**å»ºè®®**: æ”¹ç”¨ `nn.GroupNorm(num_groups=1, num_channels=C)`

---

## ğŸ”§ éœ€è¦ä¿®æ”¹çš„åœ°æ–¹

### 1ï¸âƒ£ **éŸ³é¢‘ç¼–ç å™¨** (Critical)

```python
# âŒ å½“å‰å®ç°
self.audio_encoder = nn.Sequential(
    Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # é”™è¯¯
    # ...
)

# âœ… ä¿®æ”¹ä¸º
self.audio_encoder = nn.Sequential(
    # ç¬¬1å±‚ï¼š1â†’16
    Conv2d(1, 16, kernel_size=3, stride=2, padding=0),  # æ³¨æ„ padding=0
    
    # ç¬¬2å±‚ï¼š16â†’32
    Conv2d(16, 32, kernel_size=3, stride=2, padding=0),  # æ³¨æ„ padding=0
    
    # ç¬¬3å±‚ï¼š32â†’32 + Residual
    Conv2d(32, 32, kernel_size=3, stride=1, padding=1, residual=True),
    
    # ç¬¬4å±‚ï¼š32â†’64
    Conv2d(32, 64, kernel_size=3, stride=2, padding=1),
    
    # åˆ é™¤è¿™ä¸€å±‚ âŒ
    # Conv2d(64, 64, kernel_size=3, stride=1, padding=1, residual=True),
    
    # ç¬¬5å±‚ï¼š64â†’128
    Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
    
    # ç¬¬6å±‚ï¼š128â†’128 (dilation=2)
    nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1, dilation=2),  # âš ï¸ æ·»åŠ  dilation
    nn.BatchNorm2d(128),
    nn.ReLU(),
    
    # ç¬¬7å±‚ï¼š128â†’128 (dilation=2, padding=2)
    nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=2, dilation=2),  # âš ï¸ æ·»åŠ  dilation
    nn.BatchNorm2d(128),
    nn.ReLU(),
    
    # ç¬¬8å±‚ï¼š128â†’128 + Residual
    Conv2d(128, 128, kernel_size=3, stride=1, padding=1, residual=True),
)
# âŒ åˆ é™¤ AdaptiveAvgPool2d - NCNN ä¸ä½¿ç”¨
```

### 2ï¸âƒ£ **å›¾åƒç¼–ç å™¨ç¬¬ä¸€å±‚** (Critical)

```python
# âŒ å½“å‰å®ç°
self.features = [conv_bn(6, input_channel, 2, norm_type)]  # stride=2, 32é€šé“

# âœ… ä¿®æ”¹ä¸º
self.features = [
    nn.ZeroPad2d(1),  # æ·»åŠ  padding
    nn.Conv2d(6, 16, 3, stride=1, padding=0, bias=False),  # 6â†’16, stride=1
    norm_type(16),
    nn.ReLU(inplace=True),
    
    nn.Conv2d(16, 32, 3, stride=2, padding=1, bias=False),  # 16â†’32, stride=2
    norm_type(32),
    nn.ReLU(inplace=True)
]
```

### 3ï¸âƒ£ **å½’ä¸€åŒ–å±‚** (Important)

```python
# âŒ å½“å‰å®ç°
norm_type = nn.InstanceNorm2d

# âœ… ä¿®æ”¹ä¸º
norm_type = lambda channels: nn.GroupNorm(num_groups=1, num_channels=channels)
```

### 4ï¸âƒ£ **è¾“å‡ºæ¿€æ´»å‡½æ•°** (Critical)

```python
# âŒ å½“å‰å®ç°
x = torch.sigmoid(x)

# âœ… ä¿®æ”¹ä¸º
x = torch.tanh(x)
```

### 5ï¸âƒ£ **è¾“å‡ºå±‚å‰çš„ Padding**

```python
# âœ… åœ¨æœ€åçš„å·ç§¯å‰æ·»åŠ 
x = self.dconv5(x)
x = nn.functional.pad(x, (1, 1, 1, 1), mode='constant', value=0)  # æ·»åŠ  padding
x = self.conv_last(x)
x = self.conv_score(x)
x = torch.tanh(x)
```

---

## ğŸ“‹ ä¿®æ”¹åçš„å®Œæ•´éŸ³é¢‘ç¼–ç å™¨

```python
class AudioEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, 16, 3, stride=2, padding=0),  # [B, 16, 127, 9]
            nn.BatchNorm2d(16),
            nn.ReLU()
        )
        
        self.conv2 = nn.Sequential(
            nn.Conv2d(16, 32, 3, stride=2, padding=0),  # [B, 32, 63, 4]
            nn.BatchNorm2d(32),
            nn.ReLU()
        )
        
        # Residual block
        self.conv3 = nn.Conv2d(32, 32, 3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(32)
        
        self.conv4 = nn.Sequential(
            nn.Conv2d(32, 64, 3, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU()
        )
        
        self.conv5 = nn.Sequential(
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU()
        )
        
        # Dilated convolutions
        self.conv6 = nn.Sequential(
            nn.Conv2d(128, 128, 3, stride=2, padding=1, dilation=2),
            nn.BatchNorm2d(128),
            nn.ReLU()
        )
        
        self.conv7 = nn.Sequential(
            nn.Conv2d(128, 128, 3, stride=2, padding=2, dilation=2),
            nn.BatchNorm2d(128),
            nn.ReLU()
        )
        
        # Final residual block
        self.conv8 = nn.Conv2d(128, 128, 3, stride=1, padding=1)
        self.bn8 = nn.BatchNorm2d(128)
        
    def forward(self, x):
        # x: [B, 1, 256, 20]
        x = self.conv1(x)
        x = self.conv2(x)
        
        # Residual
        identity = x
        x = self.conv3(x)
        x = self.bn3(x)
        x = x + identity
        x = F.relu(x)
        
        x = self.conv4(x)
        x = self.conv5(x)
        x = self.conv6(x)
        x = self.conv7(x)
        
        # Final residual
        identity = x
        x = self.conv8(x)
        x = self.bn8(x)
        x = x + identity
        x = F.relu(x)
        
        return x
```

---

## ğŸ“Š ä¿®æ”¹ä¼˜å…ˆçº§

| ä¼˜å…ˆçº§ | ä¿®æ”¹é¡¹ | å½±å“ç¨‹åº¦ | å¿…è¦æ€§ |
|--------|--------|----------|--------|
| ğŸ”´ P0 | è¾“å‡ºæ¿€æ´»å‡½æ•° (TanH vs Sigmoid) | æé«˜ | **å¿…é¡»** |
| ğŸ”´ P0 | éŸ³é¢‘ç¼–ç å™¨ç¬¬1-2å±‚é€šé“æ•°å’Œpadding | æé«˜ | **å¿…é¡»** |
| ğŸŸ  P1 | éŸ³é¢‘ç¼–ç å™¨ dilation å·ç§¯ | é«˜ | **å¼ºçƒˆå»ºè®®** |
| ğŸŸ  P1 | å›¾åƒç¼–ç å™¨ç¬¬ä¸€å±‚ç»“æ„ | é«˜ | **å¼ºçƒˆå»ºè®®** |
| ğŸŸ¡ P2 | å½’ä¸€åŒ–å±‚ç±»å‹ (GroupNorm) | ä¸­ | å»ºè®® |
| ğŸŸ¡ P2 | è¾“å‡ºå±‚å‰çš„ Padding | ä¸­ | å»ºè®® |
| ğŸŸ¢ P3 | åˆ é™¤ AdaptiveAvgPool2d | ä½ | å¯é€‰ |

---

## ğŸ§ª éªŒè¯æ–¹æ³•

### 1. å±‚æ•°å¯¹æ¯”

```python
# ç»Ÿè®¡ PyTorch æ¨¡å‹å±‚æ•°
def count_layers(model):
    count = 0
    for module in model.modules():
        if isinstance(module, (nn.Conv2d, nn.ConvTranspose2d, 
                               nn.BatchNorm2d, nn.GroupNorm)):
            count += 1
    return count

# NCNN æ¨¡å‹: 165 å±‚
# PyTorch æ¨¡å‹: åº”è¯¥æ¥è¿‘ 165 å±‚
```

### 2. è¾“å‡ºå½¢çŠ¶éªŒè¯

```python
model = MobileNetV2Unet()
audio = torch.randn(1, 256, 20)
face = torch.randn(1, 6, 160, 160)

output = model(face, audio)
print(f"Output shape: {output.shape}")  # åº”è¯¥æ˜¯ [1, 3, 160, 160]
print(f"Output range: [{output.min():.3f}, {output.max():.3f}]")  # åº”è¯¥æ˜¯ [-1, 1]
```

### 3. å‚æ•°é‡å¯¹æ¯”

```python
# NCNN æ¨¡å‹å‚æ•°é‡: ~3.77M
total_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {total_params / 1e6:.2f}M")
# åº”è¯¥æ¥è¿‘ 3.77M
```

---

## ğŸ“ æ€»ç»“

### ä¸»è¦é—®é¢˜

1. âŒ **éŸ³é¢‘ç¼–ç å™¨ç»“æ„é”™è¯¯**ï¼šç¼ºå°‘ 1â†’16 å±‚ï¼Œç¼ºå°‘ dilation å·ç§¯
2. âŒ **å›¾åƒç¼–ç å™¨ç¬¬ä¸€å±‚é”™è¯¯**ï¼šé€šé“æ•°ã€stride ä¸åŒ¹é…
3. âŒ **è¾“å‡ºæ¿€æ´»å‡½æ•°é”™è¯¯**ï¼šä½¿ç”¨äº† Sigmoid è€Œé TanH
4. âš ï¸ **å½’ä¸€åŒ–å±‚ç±»å‹ä¸åŒ**ï¼šInstanceNorm vs GroupNorm

### ä¿®æ”¹å»ºè®®

æŒ‰ç…§ä¸Šè¿° **ä¿®æ”¹ä¼˜å…ˆçº§** é€æ­¥è°ƒæ•´ï¼š
1. å…ˆä¿®æ”¹ **P0** çº§åˆ«çš„é—®é¢˜ï¼ˆæ¿€æ´»å‡½æ•°ã€éŸ³é¢‘ç¼–ç å™¨åŸºç¡€ç»“æ„ï¼‰
2. å†ä¿®æ”¹ **P1** çº§åˆ«çš„é—®é¢˜ï¼ˆdilationã€å›¾åƒç¼–ç å™¨ï¼‰
3. æœ€åè°ƒæ•´ **P2-P3** çº§åˆ«çš„ç»†èŠ‚

### é¢„æœŸæ•ˆæœ

å®Œæˆæ‰€æœ‰ä¿®æ”¹åï¼š
- âœ… ç½‘ç»œç»“æ„ä¸ NCNN æ¨¡å‹å®Œå…¨ä¸€è‡´
- âœ… å¯ä»¥æ­£ç¡®åŠ è½½ NCNN çš„æƒé‡
- âœ… æ¨ç†ç»“æœä¸ NCNN ä¿æŒä¸€è‡´

